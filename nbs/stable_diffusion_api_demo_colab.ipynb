{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertotrunk/depth2video/blob/main/stable_diffusion_depth2video_demo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt5s_yLzCuv8"
      },
      "source": [
        "# Basic Usage\n",
        "\n",
        "1. install client and connect to the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZumUlQICuv-"
      },
      "outputs": [],
      "source": [
        "#%pip install ..\n",
        "%pip install stability-sdk\n",
        "%pip install ffmpeg-python\n",
        "ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfLmVAJRCuv_"
      },
      "outputs": [],
      "source": [
        "import getpass, os\n",
        "\n",
        "# NB: host url is not prepended with \\\"https\\\" nor does it have a trailing slash.\n",
        "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
        "\n",
        "# To get your API key, visit https://beta.dreamstudio.ai/membership\n",
        "os.environ['STABILITY_KEY'] = getpass.getpass('Enter your API Key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW5Bygy7CuwA"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from stability_sdk import client\n",
        "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "\n",
        "\n",
        "stability_api = client.StabilityInference(\n",
        "    key=os.environ['STABILITY_KEY'], \n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM4E0ehsCuwA"
      },
      "source": [
        "2. Submit a request to generate a single image from a text prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import ffmpeg\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "\n",
        "in_filename = '/content/sample_data/input.mp4'\n",
        "out_filename = '/content/out.mp4'\n",
        "width, height = 512, 512\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def get_video_size(filename):\n",
        "    logger.info('Getting video size for {!r}'.format(filename))\n",
        "    probe = ffmpeg.probe(filename)\n",
        "    video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')\n",
        "    width = int(video_info['width'])\n",
        "    height = int(video_info['height'])\n",
        "    return width, height\n",
        "\n",
        "\n",
        "def start_ffmpeg_process1(in_filename):\n",
        "    logger.info('Starting ffmpeg process1')\n",
        "    args = (\n",
        "        ffmpeg\n",
        "        .input(in_filename)\n",
        "        .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
        "        .compile()\n",
        "    )\n",
        "    return subprocess.Popen(args, stdout=subprocess.PIPE)\n",
        "\n",
        "\n",
        "def start_ffmpeg_process2(out_filename, width, height):\n",
        "    logger.info('Starting ffmpeg process2')\n",
        "    args = (\n",
        "        ffmpeg\n",
        "        .input('pipe:', format='rawvideo', pix_fmt='rgb24', s='{}x{}'.format(width, height))\n",
        "        .output(out_filename, pix_fmt='yuv420p')\n",
        "        .overwrite_output()\n",
        "        .compile()\n",
        "    )\n",
        "    return subprocess.Popen(args, stdin=subprocess.PIPE)\n",
        "\n",
        "\n",
        "def read_frame(process1, width, height):\n",
        "    logger.debug('Reading frame')\n",
        "\n",
        "    # Note: RGB24 == 3 bytes per pixel.\n",
        "    frame_size = width * height * 3\n",
        "    in_bytes = process1.stdout.read(frame_size)\n",
        "    if len(in_bytes) == 0:\n",
        "        frame = None\n",
        "    else:\n",
        "        assert len(in_bytes) == frame_size\n",
        "        frame = (\n",
        "            np\n",
        "            .frombuffer(in_bytes, np.uint8)\n",
        "            .reshape([height, width, 3])\n",
        "        )\n",
        "    return frame\n",
        "\n",
        "\n",
        "def process_frame_simple(frame):\n",
        "    '''Simple processing example: darken frame.'''\n",
        "    return frame * 0.3\n",
        "\n",
        "\n",
        "def write_frame(process2, frame):\n",
        "    logger.debug('Writing frame')\n",
        "    process2.stdin.write(\n",
        "        frame\n",
        "        .astype(np.uint8)\n",
        "        .tobytes()\n",
        "    )\n",
        "\n",
        "\n",
        "def run(in_filename, out_filename, process_frame):\n",
        "    width, height = get_video_size(in_filename)\n",
        "    process1 = start_ffmpeg_process1(in_filename)\n",
        "    process2 = start_ffmpeg_process2(out_filename, width, height)\n",
        "    while True:\n",
        "        in_frame = read_frame(process1, width, height)\n",
        "        if in_frame is None:\n",
        "            logger.info('End of input stream')\n",
        "            break\n",
        "\n",
        "        logger.debug('Processing frame')\n",
        "        out_frame = process_frame(in_frame)\n",
        "        write_frame(process2, out_frame)\n",
        "\n",
        "    logger.info('Waiting for ffmpeg process1')\n",
        "    process1.wait()\n",
        "\n",
        "    logger.info('Waiting for ffmpeg process2')\n",
        "    process2.stdin.close()\n",
        "    process2.wait()\n",
        "\n",
        "    logger.info('Done')\n",
        "\n",
        "\n",
        "class DeepDream(object):\n",
        "    '''DeepDream implementation, adapted from official tensorflow deepdream tutorial:\n",
        "    https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream\n",
        "    Credit: Alexander Mordvintsev\n",
        "    '''\n",
        "\n",
        "    _DOWNLOAD_URL = 'https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip'\n",
        "    _ZIP_FILENAME = 'deepdream_model.zip'\n",
        "    _MODEL_FILENAME = 'tensorflow_inception_graph.pb'\n",
        "\n",
        "    @staticmethod\n",
        "    def _download_model():\n",
        "        logger.info('Downloading deepdream model...')\n",
        "        try:\n",
        "            from urllib.request import urlretrieve  # python 3\n",
        "        except ImportError:\n",
        "            from urllib import urlretrieve  # python 2\n",
        "        urlretrieve(DeepDream._DOWNLOAD_URL, DeepDream._ZIP_FILENAME)\n",
        "\n",
        "        logger.info('Extracting deepdream model...')\n",
        "        zipfile.ZipFile(DeepDream._ZIP_FILENAME, 'r').extractall('.')\n",
        "\n",
        "    @staticmethod\n",
        "    def _tffunc(*argtypes):\n",
        "        '''Helper that transforms TF-graph generating function into a regular one.\n",
        "        See `_resize` function below.\n",
        "        '''\n",
        "        placeholders = list(map(tf.placeholder, argtypes))\n",
        "        def wrap(f):\n",
        "            out = f(*placeholders)\n",
        "            def wrapper(*args, **kw):\n",
        "                return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n",
        "            return wrapper\n",
        "        return wrap\n",
        "\n",
        "    @staticmethod\n",
        "    def _base_resize(img, size):\n",
        "        '''Helper function that uses TF to resize an image'''\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return tf.image.resize_bilinear(img, size)[0,:,:,:]\n",
        "\n",
        "    def __init__(self):\n",
        "        if not os.path.exists(DeepDream._MODEL_FILENAME):\n",
        "            self._download_model()\n",
        "\n",
        "        self._graph = tf.Graph()\n",
        "        self._session = tf.InteractiveSession(graph=self._graph)\n",
        "        self._resize = self._tffunc(np.float32, np.int32)(self._base_resize)\n",
        "        with tf.gfile.FastGFile(DeepDream._MODEL_FILENAME, 'rb') as f:\n",
        "            graph_def = tf.GraphDef()\n",
        "            graph_def.ParseFromString(f.read())\n",
        "        self._t_input = tf.placeholder(np.float32, name='input') # define the input tensor\n",
        "        imagenet_mean = 117.0\n",
        "        t_preprocessed = tf.expand_dims(self._t_input-imagenet_mean, 0)\n",
        "        tf.import_graph_def(graph_def, {'input':t_preprocessed})\n",
        "\n",
        "        self.t_obj = self.T('mixed4d_3x3_bottleneck_pre_relu')[:,:,:,139]\n",
        "        #self.t_obj = tf.square(self.T('mixed4c'))\n",
        "\n",
        "    def T(self, layer_name):\n",
        "        '''Helper for getting layer output tensor'''\n",
        "        return self._graph.get_tensor_by_name('import/%s:0'%layer_name)\n",
        "\n",
        "    def _calc_grad_tiled(self, img, t_grad, tile_size=512):\n",
        "        '''Compute the value of tensor t_grad over the image in a tiled way.\n",
        "        Random shifts are applied to the image to blur tile boundaries over \n",
        "        multiple iterations.'''\n",
        "        sz = tile_size\n",
        "        h, w = img.shape[:2]\n",
        "        sx, sy = np.random.randint(sz, size=2)\n",
        "        img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n",
        "        grad = np.zeros_like(img)\n",
        "        for y in range(0, max(h-sz//2, sz),sz):\n",
        "            for x in range(0, max(w-sz//2, sz),sz):\n",
        "                sub = img_shift[y:y+sz,x:x+sz]\n",
        "                g = self._session.run(t_grad, {self._t_input:sub})\n",
        "                grad[y:y+sz,x:x+sz] = g\n",
        "        return np.roll(np.roll(grad, -sx, 1), -sy, 0)\n",
        "\n",
        "    def process_frame(self, frame, iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n",
        "        t_score = tf.reduce_mean(self.t_obj) # defining the optimization objective\n",
        "        t_grad = tf.gradients(t_score, self._t_input)[0] # behold the power of automatic differentiation!\n",
        "\n",
        "        # split the image into a number of octaves\n",
        "        img = frame\n",
        "        octaves = []\n",
        "        for i in range(octave_n-1):\n",
        "            hw = img.shape[:2]\n",
        "            lo = self._resize(img, np.int32(np.float32(hw)/octave_scale))\n",
        "            hi = img-self._resize(lo, hw)\n",
        "            img = lo\n",
        "            octaves.append(hi)\n",
        "        \n",
        "        # generate details octave by octave\n",
        "        for octave in range(octave_n):\n",
        "            if octave>0:\n",
        "                hi = octaves[-octave]\n",
        "                img = self._resize(img, hi.shape[:2])+hi\n",
        "            for i in range(iter_n):\n",
        "                g = self._calc_grad_tiled(img, t_grad)\n",
        "                img += g*(step / (np.abs(g).mean()+1e-7))\n",
        "                #print('.',end = ' ')\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "process_frame = DeepDream().process_frame"
      ],
      "metadata": {
        "id": "0vVGea4dNez6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkdNb3v9CuwA"
      },
      "outputs": [],
      "source": [
        "# the object returned is a python generator\n",
        "answers = stability_api.generate(\n",
        "    prompt=\"houston, we are a 'go' for launch!\",\n",
        "    seed=34567, # if provided, specifying a random seed makes results deterministic\n",
        "    steps=20, # defaults to 30 if not specified\n",
        ")\n",
        "\n",
        "# iterating over the generator produces the api response\n",
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img = Image.open('/content/sample_data/01_c.png')\n",
        "            display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxbTcxbhCuwB"
      },
      "source": [
        "# Intermediate usage\n",
        "\n",
        "3. (new!) An \"init\" image can be provided for text-driven image modification. To demonstrate, we can convert the image we just generated to a colored pencil sketch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Ld7yyQCuwC"
      },
      "outputs": [],
      "source": [
        "answers = stability_api.generate(\n",
        "    prompt=\"childrens crayon drawing of a rocket launch\",\n",
        "    init_image=img,\n",
        "    seed=54321, # if we're passing in an image generated by SD, you may get better results by providing a different seed value than was used to generate the image\n",
        "    start_schedule=0.6, # this controls the \"strength\" of the prompt relative to the init image\n",
        ")\n",
        "\n",
        "# iterating over the generator produces the api response\n",
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img2 = img\n",
        "            display(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQyTnMbKCuwC"
      },
      "outputs": [],
      "source": [
        "# we can make a rough mask by thresholding the grayscaled image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "img2_grayscale = Image.open('/content/sample_data/01_m.png').convert('L')\n",
        "img2_a = np.array(img2_grayscale)\n",
        "\n",
        "mask = np.array(img2_grayscale)\n",
        "strength = .2  # this controls the \"strength\" of the prompt relative to the init image\n",
        "\n",
        "d = int(255 * (1-strength))\n",
        "mask *= 255-d # convert from range [0,1] to [0,255]\n",
        "mask += d\n",
        "\n",
        "mask = Image.fromarray(mask)\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97E6HeL3CuwD"
      },
      "outputs": [],
      "source": [
        "# fuzzing the mask edges generally improves synthesis results\n",
        "\n",
        "from torchvision.transforms import GaussianBlur\n",
        "\n",
        "blur = GaussianBlur(11,20)\n",
        "mask = blur(mask)\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiEtrDHNCuwD"
      },
      "outputs": [],
      "source": [
        "# now, let's send our drawing into space\n",
        "answers = stability_api.generate(\n",
        "    prompt=\"scarlett johansson\",\n",
        "    init_image=img2,\n",
        "    mask_image=mask,\n",
        "    seed=12345, # if we're passing in an image generated by SD, you may get better results by providing a different seed value than was used to generate the image\n",
        "    start_schedule=1,\n",
        ")\n",
        "\n",
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img3 = Image.open(io.BytesIO(artifact.binary))\n",
        "            display(img3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUz_eBrQCuwD"
      },
      "outputs": [],
      "source": [
        "# we can improve our image result by introducing clip guidance into the request. this takes longer (and costs more compute) but often yields better results.\n",
        "# clip guidance is built with several presets optimized for speed or quality, but clip can be controlled more precisely with a variety of other params\n",
        "\n",
        "answers = stability_api.generate(\n",
        "    prompt=\"houston, we are a 'go' for launch!\",\n",
        "    seed=34567,  \n",
        "    steps=35, # minimum of 35 steps recommended when using CLIP\n",
        "    guidance_preset = generation.GUIDANCE_PRESET_FAST_BLUE\n",
        ")\n",
        "\n",
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img4 = Image.open(io.BytesIO(artifact.binary))\n",
        "            print('GUIDANCE: FAST-BLUE:')\n",
        "            display(img4)\n",
        "\n",
        "answers = stability_api.generate(\n",
        "    prompt=\"houston, we are a 'go' for launch!\",\n",
        "    seed=34567, \n",
        "    steps=35, \n",
        "    guidance_preset = generation.GUIDANCE_PRESET_SLOWER\n",
        ")\n",
        "\n",
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img5 = Image.open(io.BytesIO(artifact.binary))\n",
        "            print('GUIDANCE: SLOWER:')\n",
        "            display(img5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('dmarx-je5LfYh2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "57881a85d677a34ea29564e0084ef84f4058c4e30a2bb466eb0e0b908d0628df"
      }
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}